{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Compose, RandomResizedCrop, HorizontalFlip, RandomBrightnessContrast,\n",
    "    Normalize, ShiftScaleRotate, CoarseDropout, ColorJitter, RandomGamma, GaussianBlur,\n",
    "    OneOf\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Содержимое data_root: ['.DS_Store', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "# Устанавливаем необходимые библиотеки, если они не установлены\n",
    "# !pip install albumentations --quiet\n",
    "# !pip install timm --quiet\n",
    "\n",
    "# Используем timm для удобного доступа к современным моделям\n",
    "import timm\n",
    "\n",
    "# Определение классов\n",
    "class_names = ['cleaned', 'dirty']\n",
    "\n",
    "# Корневой каталог данных\n",
    "data_root = 'Data/plates'\n",
    "print(\"Содержимое data_root:\", os.listdir(data_root))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Содержимое data_root: ['test', 'train']\n",
      "Количество обучающих изображений: 40\n",
      "Количество валидационных изображений: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\timm\\models\\_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b4_ns to current tf_efficientnet_b4.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from albumentations import (\n",
    "    Compose, RandomResizedCrop, HorizontalFlip, VerticalFlip, RandomBrightnessContrast,\n",
    "    Normalize, ShiftScaleRotate, CoarseDropout, CenterCrop, OneOf,\n",
    "    HueSaturationValue, ToGray, GaussNoise, MotionBlur,\n",
    "    MedianBlur, Blur, CLAHE, RandomRotate90, Transpose, GridDistortion, OpticalDistortion,\n",
    "    ElasticTransform\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from PIL import Image\n",
    "\n",
    "# Установка необходимых библиотек, если они не установлены\n",
    "# !pip install albumentations timm\n",
    "\n",
    "import timm  # Библиотека с множеством современных моделей\n",
    "from timm.data import Mixup\n",
    "from timm.loss import SoftTargetCrossEntropy\n",
    "\n",
    "# Определение классов\n",
    "class_names = ['cleaned', 'dirty']\n",
    "\n",
    "# Корневой каталог данных\n",
    "data_root = 'Data/plates'\n",
    "print(\"Содержимое data_root:\", os.listdir(data_root))\n",
    "\n",
    "train_dir = 'drive/MyDrive/plates/train'\n",
    "val_dir = 'drive/MyDrive/plates/val'\n",
    "\n",
    "# Проверяем и создаем необходимые директории\n",
    "for dir_name in [train_dir, val_dir]:\n",
    "    for class_name in class_names:\n",
    "        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n",
    "\n",
    "# Функция для копирования и разделения данных\n",
    "def split_data(source_dir, train_dir, val_dir, split_ratio=0.9):\n",
    "    files = os.listdir(source_dir)\n",
    "    files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    np.random.shuffle(files)\n",
    "    split_idx = int(len(files) * split_ratio)\n",
    "    train_files = files[:split_idx]\n",
    "    val_files = files[split_idx:]\n",
    "    \n",
    "    for file_name in train_files:\n",
    "        src_file = os.path.join(source_dir, file_name)\n",
    "        dest_file = os.path.join(train_dir, file_name)\n",
    "        # Копируем только если файл не существует в целевой директории\n",
    "        if not os.path.exists(dest_file):\n",
    "            shutil.copy(src_file, dest_file)\n",
    "    \n",
    "    for file_name in val_files:\n",
    "        src_file = os.path.join(source_dir, file_name)\n",
    "        dest_file = os.path.join(val_dir, file_name)\n",
    "        if not os.path.exists(dest_file):\n",
    "            shutil.copy(src_file, dest_file)\n",
    "\n",
    "# Разделяем данные на обучающую и валидационную выборки\n",
    "for class_name in class_names:\n",
    "    source_dir = os.path.join(data_root, 'train', class_name)\n",
    "    train_class_dir = os.path.join(train_dir, class_name)\n",
    "    val_class_dir = os.path.join(val_dir, class_name)\n",
    "    split_data(source_dir, train_class_dir, val_class_dir, split_ratio=0.9)\n",
    "\n",
    "# Продвинутые аугментации с Albumentations\n",
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "        RandomResizedCrop(380, 380),\n",
    "        Transpose(p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        VerticalFlip(p=0.5),\n",
    "        ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=45, p=0.5),\n",
    "        HueSaturationValue(p=0.5),\n",
    "        RandomBrightnessContrast(p=0.5),\n",
    "        RandomRotate90(p=0.5),\n",
    "        OneOf([\n",
    "            ElasticTransform(p=0.5),\n",
    "            GridDistortion(p=0.5),\n",
    "            OpticalDistortion(p=0.5),\n",
    "        ], p=0.5),\n",
    "        OneOf([\n",
    "            GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "            MotionBlur(p=0.5),\n",
    "            MedianBlur(blur_limit=5, p=0.5),\n",
    "            Blur(blur_limit=5, p=0.5),\n",
    "        ], p=0.5),\n",
    "        CoarseDropout(max_holes=8, max_height=40, max_width=40, fill_value=0, p=0.5),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_val_transforms():\n",
    "    return Compose([\n",
    "        CenterCrop(380, 380),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "# Кастомный датасет для использования с Albumentations\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка загрузки изображения {self.image_paths[idx]}: {e}\")\n",
    "            image = np.zeros((380, 380, 3), dtype=np.uint8)\n",
    "        label = self.labels[idx]\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image)\n",
    "            image = augmented['image']\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "# Собираем пути к изображениям и метки\n",
    "def get_image_paths_and_labels(data_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            if file_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(class_dir, file_name))\n",
    "                labels.append(label_idx)\n",
    "    return image_paths, labels\n",
    "\n",
    "train_image_paths, train_labels = get_image_paths_and_labels(train_dir)\n",
    "val_image_paths, val_labels = get_image_paths_and_labels(val_dir)\n",
    "\n",
    "print(f\"Количество обучающих изображений: {len(train_image_paths)}\")\n",
    "print(f\"Количество валидационных изображений: {len(val_image_paths)}\")\n",
    "\n",
    "# Создаем датасеты и загрузчики данных\n",
    "batch_size = 8  # Увеличение batch_size при наличии ресурсов\n",
    "\n",
    "train_dataset = CustomImageDataset(train_image_paths, train_labels, transforms=get_train_transforms())\n",
    "val_dataset = CustomImageDataset(val_image_paths, val_labels, transforms=get_val_transforms())\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Используем модель EfficientNet-B4 из timm\n",
    "model = timm.create_model('tf_efficientnet_b4_ns', pretrained=True, num_classes=len(class_names))\n",
    "\n",
    "# Используем GPU, если доступен\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Используем Mixup для аугментации данных\n",
    "mixup_fn = Mixup(\n",
    "    mixup_alpha=0.2,\n",
    "    cutmix_alpha=0.2,\n",
    "    cutmix_minmax=None,\n",
    "    prob=1.0,\n",
    "    switch_prob=0.5,\n",
    "    mode='batch',\n",
    "    label_smoothing=0.1,\n",
    "    num_classes=len(class_names)\n",
    ")\n",
    "\n",
    "# Используем Soft Target Cross Entropy Loss\n",
    "loss_fn = SoftTargetCrossEntropy()\n",
    "\n",
    "# Оптимизатор и планировщик\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Используем смешанную точность\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Функция обучения\n",
    "def train_model(model, loss_fn, optimizer, scheduler, num_epochs, device):\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                dataloader = train_dataloader\n",
    "            else:\n",
    "                model.eval()\n",
    "                dataloader = val_dataloader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in tqdm(dataloader, desc=phase):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if phase == 'train':\n",
    "                    inputs, labels = mixup_fn(inputs, labels)\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(inputs)\n",
    "                        loss = loss_fn(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                if phase == 'val':\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    running_corrects += torch.sum(preds == labels.argmax(dim=1))\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            if phase == 'val':\n",
    "                epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "        print()\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "num_epochs = 20\n",
    "train_model(model, loss_fn, optimizer, scheduler, num_epochs, device)\n",
    "\n",
    "# Загружаем лучшую модель\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Подготовка тестовых данных\n",
    "test_dir = os.path.join(data_root, 'test')\n",
    "test_image_paths = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir) if fname.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "print(f\"Количество тестовых изображений: {len(test_image_paths)}\")\n",
    "\n",
    "test_dataset = CustomImageDataset(test_image_paths, [0]*len(test_image_paths), transforms=get_val_transforms())\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Предсказания на тестовых данных с использованием TTA\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "for inputs, _ in tqdm(test_dataloader, desc='Test'):\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            probs = nn.functional.softmax(outputs, dim=1)[:, 1]  # Вероятность класса 'dirty'\n",
    "    test_predictions.extend(probs.cpu().numpy())\n",
    "\n",
    "# Создание DataFrame для сабмита\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': [os.path.basename(path).replace('.jpg', '').replace('.jpeg', '').replace('.png', '') for path in test_image_paths],\n",
    "    'label': ['dirty' if pred > 0.5 else 'cleaned' for pred in test_predictions]\n",
    "})\n",
    "\n",
    "# Сортировка и сохранение\n",
    "submission_df = submission_df.sort_values('id')\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Файл submission.csv сохранен.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Содержимое data_root: ['cleaned', 'dirty', 'test', 'train', 'val']\n",
      "Количество обучающих изображений: 40\n",
      "Количество валидационных изображений: 25\n",
      "Epoch 1/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 10.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.7188 Acc: 0.5250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 4/4 [00:00<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7263 Acc: 0.4800\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 24.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6752 Acc: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 4/4 [00:00<00:00, 64.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7008 Acc: 0.5600\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 23.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6298 Acc: 0.6750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 4/4 [00:00<00:00, 59.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5837 Acc: 0.7600\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 20.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6072 Acc: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 4/4 [00:00<00:00, 60.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5542 Acc: 0.8000\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 24.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5871 Acc: 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 4/4 [00:00<00:00, 51.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5030 Acc: 0.8400\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 24.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5081 Acc: 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 4/4 [00:00<00:00, 49.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.4543 Acc: 0.8800\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 23.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4922 Acc: 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 4/4 [00:00<00:00, 64.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.4037 Acc: 0.9600\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 23.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4278 Acc: 0.9250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 4/4 [00:00<00:00, 58.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3641 Acc: 1.0000\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 21.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4950 Acc: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 4/4 [00:00<00:00, 61.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3386 Acc: 0.9200\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 20.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3967 Acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 4/4 [00:00<00:00, 61.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3129 Acc: 0.9200\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 23.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3817 Acc: 0.8500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 4/4 [00:00<00:00, 63.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2712 Acc: 1.0000\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 23.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3352 Acc: 0.9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 4/4 [00:00<00:00, 66.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2460 Acc: 0.9200\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 22.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2809 Acc: 0.9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 4/4 [00:00<00:00, 61.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2367 Acc: 0.9600\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 24.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2575 Acc: 0.9750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 4/4 [00:00<00:00, 55.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2093 Acc: 1.0000\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 24.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3407 Acc: 0.9250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 4/4 [00:00<00:00, 51.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1470 Acc: 1.0000\n",
      "Early stopping at epoch 15\n",
      "Количество тестовых изображений: 744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 93/93 [00:01<00:00, 47.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл submission.csv сохранен.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from albumentations import (\n",
    "    Compose, RandomResizedCrop, HorizontalFlip, RandomBrightnessContrast,\n",
    "    Normalize, ShiftScaleRotate, CoarseDropout\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from PIL import Image\n",
    "\n",
    "# Убедитесь, что все необходимые библиотеки установлены\n",
    "# !pip install albumentations efficientnet_pytorch\n",
    "\n",
    "class_names = ['cleaned', 'dirty']\n",
    "\n",
    "data_root = 'drive/MyDrive/plates'\n",
    "print(\"Содержимое data_root:\", os.listdir(data_root))\n",
    "\n",
    "train_dir = 'drive/MyDrive/plates'\n",
    "val_dir = 'val'\n",
    "\n",
    "# Проверяем и создаем необходимые директории\n",
    "for dir_name in [train_dir, val_dir]:\n",
    "    for class_name in class_names:\n",
    "        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n",
    "\n",
    "# Функция для копирования и разделения данных\n",
    "def split_data(source_dir, train_dir, val_dir, split_ratio=0.85):\n",
    "    files = os.listdir(source_dir)\n",
    "    files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    np.random.shuffle(files)\n",
    "    split_idx = int(len(files) * split_ratio)\n",
    "    train_files = files[:split_idx]\n",
    "    val_files = files[split_idx:]\n",
    "    for file_name in train_files:\n",
    "        shutil.copy(os.path.join(source_dir, file_name), os.path.join(train_dir, file_name))\n",
    "    for file_name in val_files:\n",
    "        shutil.copy(os.path.join(source_dir, file_name), os.path.join(val_dir, file_name))\n",
    "\n",
    "# Разделяем данные на обучающую и валидационную выборки\n",
    "for class_name in class_names:\n",
    "    source_dir = os.path.join(data_root, 'train', class_name)\n",
    "    train_class_dir = os.path.join(train_dir, class_name)\n",
    "    val_class_dir = os.path.join(val_dir, class_name)\n",
    "    split_data(source_dir, train_class_dir, val_class_dir, split_ratio=0.85)\n",
    "\n",
    "# Продвинутые аугментации с Albumentations\n",
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "        RandomResizedCrop(224, 224),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        RandomBrightnessContrast(p=0.5),\n",
    "        CoarseDropout(max_holes=8, max_height=16, max_width=16, fill_value=0, p=0.5),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_val_transforms():\n",
    "    return Compose([\n",
    "        RandomResizedCrop(224, 224),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "# Кастомный датасет для использования с Albumentations\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка загрузки изображения {self.image_paths[idx]}: {e}\")\n",
    "            # Возвращаем пустое изображение и метку 0 (можно изменить по необходимости)\n",
    "            image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        label = self.labels[idx]\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image)\n",
    "            image = augmented['image']\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "# Собираем пути к изображениям и метки\n",
    "def get_image_paths_and_labels(data_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            if file_name.lower().endswith(('.jpg', '.jpeg', '.png')):  # Фильтрация по типу файла\n",
    "                image_paths.append(os.path.join(class_dir, file_name))\n",
    "                labels.append(label_idx)\n",
    "    return image_paths, labels\n",
    "\n",
    "train_image_paths, train_labels = get_image_paths_and_labels(train_dir)\n",
    "val_image_paths, val_labels = get_image_paths_and_labels(val_dir)\n",
    "\n",
    "print(f\"Количество обучающих изображений: {len(train_image_paths)}\")\n",
    "print(f\"Количество валидационных изображений: {len(val_image_paths)}\")\n",
    "\n",
    "# Создаем датасеты и загрузчики данных\n",
    "batch_size = 8  # Уменьшение batch_size для экономии памяти\n",
    "\n",
    "train_dataset = CustomImageDataset(train_image_paths, train_labels, transforms=get_train_transforms())\n",
    "val_dataset = CustomImageDataset(val_image_paths, val_labels, transforms=get_val_transforms())\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Используем модель EfficientNet-B0 (менее ресурсоемкая) из torchvision\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(num_ftrs, len(class_names))\n",
    ")\n",
    "\n",
    "# Используем GPU, если доступен\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Используем взвешенную CrossEntropyLoss, если классы несбалансированы\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(train_labels)\n",
    "class_weights = 1. / torch.tensor([counter[i] for i in range(len(class_names))], dtype=torch.float)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Оптимизатор и планировщик\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, verbose=True)\n",
    "\n",
    "# Используем смешанную точность (Mixed Precision) для экономии памяти\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Функция обучения с ранним прекращением\n",
    "def train_model(model, loss_fn, optimizer, scheduler, num_epochs, device, patience=7):\n",
    "    best_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                dataloader = train_dataloader\n",
    "            else:\n",
    "                model.eval()\n",
    "                dataloader = val_dataloader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in tqdm(dataloader, desc=phase):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = loss_fn(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Шаг планировщика только для валидации\n",
    "            if phase == 'val':\n",
    "                scheduler.step(epoch_loss)\n",
    "\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    epochs_no_improve = 0\n",
    "                    # Сохраняем лучшую модель\n",
    "                    torch.save(model.state_dict(), 'best_model.pth')\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch + 1}')\n",
    "                    return\n",
    "        print()\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "num_epochs = 30\n",
    "train_model(model, loss_fn, optimizer, scheduler, num_epochs, device)\n",
    "\n",
    "# Загружаем лучшую модель\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Подготовка тестовых данных\n",
    "test_dir = os.path.join(data_root, 'test')\n",
    "test_image_paths = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir) if fname.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "print(f\"Количество тестовых изображений: {len(test_image_paths)}\")\n",
    "\n",
    "test_dataset = CustomImageDataset(test_image_paths, [0]*len(test_image_paths), transforms=get_val_transforms())\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Предсказания на тестовых данных\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "for inputs, _ in tqdm(test_dataloader, desc='Test'):\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            probs = nn.functional.softmax(outputs, dim=1)[:, 1]  # Вероятность класса 'dirty'\n",
    "    test_predictions.extend(probs.cpu().numpy())\n",
    "\n",
    "# Создание DataFrame для сабмита\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': [os.path.basename(path).replace('.jpg', '').replace('.jpeg', '').replace('.png', '') for path in test_image_paths],\n",
    "    'label': ['dirty' if pred > 0.5 else 'cleaned' for pred in test_predictions]\n",
    "})\n",
    "\n",
    "# Сортировка и сохранение\n",
    "submission_df = submission_df.sort_values('id')\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Файл submission.csv сохранен.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\alexx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from efficientnet_pytorch) (2.1.2+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\alexx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\alexx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\alexx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\alexx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alexx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alexx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alexx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\alexx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
      "Building wheels for collected packages: efficientnet_pytorch\n",
      "  Building wheel for efficientnet_pytorch (setup.py): started\n",
      "  Building wheel for efficientnet_pytorch (setup.py): finished with status 'done'\n",
      "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16464 sha256=fb0b616673aeb070121066c1e9f6d99a767de45679cb0b53b9c00c026551b504\n",
      "  Stored in directory: c:\\users\\alexx\\appdata\\local\\pip\\cache\\wheels\\03\\3f\\e9\\911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
      "Successfully built efficientnet_pytorch\n",
      "Installing collected packages: efficientnet_pytorch\n",
      "Successfully installed efficientnet_pytorch-0.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Содержимое data_root: ['cleaned', 'dirty', 'test', 'train', 'val']\n",
      "Количество обучающих изображений: 40\n",
      "Количество валидационных изображений: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexx\\AppData\\Local\\Temp\\ipykernel_8316\\901042687.py:71: UserWarning: Argument 'alpha_affine' is not valid and will be ignored.\n",
      "  ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.7309 Acc: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 21.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5823 Acc: 0.7917\n",
      "\n",
      "Epoch 2/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 17.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5744 Acc: 0.6750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 53.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5152 Acc: 0.7500\n",
      "\n",
      "Epoch 3/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 18.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5450 Acc: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 53.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.4584 Acc: 0.7500\n",
      "\n",
      "Epoch 4/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 17.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5208 Acc: 0.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 44.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3358 Acc: 0.8750\n",
      "\n",
      "Epoch 5/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 17.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3192 Acc: 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 54.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2719 Acc: 0.9167\n",
      "\n",
      "Epoch 6/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 17.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2530 Acc: 0.9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 40.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2490 Acc: 0.8750\n",
      "\n",
      "Epoch 7/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 17.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4659 Acc: 0.8500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 56.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2530 Acc: 0.8750\n",
      "\n",
      "Epoch 8/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 17.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3551 Acc: 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 54.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1392 Acc: 0.9167\n",
      "\n",
      "Epoch 9/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 18.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1851 Acc: 0.9250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 55.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0387 Acc: 1.0000\n",
      "\n",
      "Epoch 10/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 17.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2070 Acc: 0.9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 53.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0321 Acc: 1.0000\n",
      "\n",
      "Epoch 11/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 17.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2178 Acc: 0.9250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 44.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0329 Acc: 1.0000\n",
      "\n",
      "Epoch 12/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 17.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1161 Acc: 0.9750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 52.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0724 Acc: 0.9583\n",
      "\n",
      "Epoch 13/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 16.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2016 Acc: 0.9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 50.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0344 Acc: 1.0000\n",
      "\n",
      "Epoch 14/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 16.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1320 Acc: 0.9750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 51.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0418 Acc: 1.0000\n",
      "\n",
      "Epoch 15/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 17.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3099 Acc: 0.9250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 53.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0643 Acc: 0.9583\n",
      "\n",
      "Epoch 16/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 5/5 [00:00<00:00, 17.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0428 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 3/3 [00:00<00:00, 55.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2753 Acc: 0.9583\n",
      "Early stopping at epoch 16\n",
      "Best val Acc: 1.0000\n",
      "Количество тестовых изображений: 744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 744/744 [00:07<00:00, 97.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл submission.csv сохранен.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from albumentations import (\n",
    "    Compose, RandomResizedCrop, HorizontalFlip, VerticalFlip, RandomBrightnessContrast,\n",
    "    Normalize, ShiftScaleRotate, CoarseDropout, ColorJitter, GaussianBlur,\n",
    "    GridDistortion, ElasticTransform, RandomGamma, HueSaturationValue, CLAHE, Resize, CenterCrop\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from PIL import Image\n",
    "\n",
    "# Убедитесь, что все необходимые библиотеки установлены\n",
    "# !pip install albumentations efficientnet_pytorch\n",
    "\n",
    "class_names = ['cleaned', 'dirty']\n",
    "\n",
    "data_root = 'drive/MyDrive/plates'\n",
    "print(\"Содержимое data_root:\", os.listdir(data_root))\n",
    "\n",
    "train_dir = os.path.join(data_root, 'train')\n",
    "val_dir = os.path.join(data_root, 'val')\n",
    "\n",
    "# Проверяем и создаем необходимые директории\n",
    "for dir_name in [train_dir, val_dir]:\n",
    "    for class_name in class_names:\n",
    "        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n",
    "\n",
    "# Функция для копирования и разделения данных\n",
    "def split_data(source_dir, train_dir, val_dir, split_ratio=0.85):\n",
    "    files = os.listdir(source_dir)\n",
    "    files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    np.random.shuffle(files)\n",
    "    split_idx = int(len(files) * split_ratio)\n",
    "    train_files = files[:split_idx]\n",
    "    val_files = files[split_idx:]\n",
    "    for file_name in train_files:\n",
    "        shutil.copy(os.path.join(source_dir, file_name), os.path.join(train_dir, file_name))\n",
    "    for file_name in val_files:\n",
    "        shutil.copy(os.path.join(source_dir, file_name), os.path.join(val_dir, file_name))\n",
    "\n",
    "# Разделяем данные на обучающую и валидационную выборки\n",
    "for class_name in class_names:\n",
    "    source_dir = os.path.join(data_root, class_name)\n",
    "    train_class_dir = os.path.join(train_dir, class_name)\n",
    "    val_class_dir = os.path.join(val_dir, class_name)\n",
    "    split_data(source_dir, train_class_dir, val_class_dir, split_ratio=0.85)\n",
    "\n",
    "# Обновленные аугментации с Albumentations\n",
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "        RandomResizedCrop(224, 224, scale=(0.8, 1.0)),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        VerticalFlip(p=0.5),\n",
    "        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=20, p=0.7),\n",
    "        RandomBrightnessContrast(p=0.7),\n",
    "        ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5),\n",
    "        GaussianBlur(blur_limit=(3,5), p=0.5),\n",
    "        RandomGamma(p=0.5),\n",
    "        HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "        CLAHE(clip_limit=4.0, p=0.5),\n",
    "        GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5),\n",
    "        CoarseDropout(max_holes=8, max_height=16, max_width=16, fill_value=0, p=0.5),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_val_transforms():\n",
    "    return Compose([\n",
    "        Resize(256, 256),\n",
    "        CenterCrop(224, 224),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "# Кастомный датасет для использования с Albumentations\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка загрузки изображения {self.image_paths[idx]}: {e}\")\n",
    "            # Возвращаем пустое изображение и метку 0\n",
    "            image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        label = self.labels[idx]\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image)\n",
    "            image = augmented['image']\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "# Собираем пути к изображениям и метки\n",
    "def get_image_paths_and_labels(data_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            if file_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(class_dir, file_name))\n",
    "                labels.append(label_idx)\n",
    "    return image_paths, labels\n",
    "\n",
    "train_image_paths, train_labels = get_image_paths_and_labels(train_dir)\n",
    "val_image_paths, val_labels = get_image_paths_and_labels(val_dir)\n",
    "\n",
    "print(f\"Количество обучающих изображений: {len(train_image_paths)}\")\n",
    "print(f\"Количество валидационных изображений: {len(val_image_paths)}\")\n",
    "\n",
    "# Создаем датасеты и загрузчики данных\n",
    "batch_size = 8  # Можно настроить в зависимости от доступной памяти\n",
    "\n",
    "train_dataset = CustomImageDataset(train_image_paths, train_labels, transforms=get_train_transforms())\n",
    "val_dataset = CustomImageDataset(val_image_paths, val_labels, transforms=get_val_transforms())\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Используем модель EfficientNet-B0\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5, inplace=True),\n",
    "    nn.Linear(num_ftrs, len(class_names))\n",
    ")\n",
    "\n",
    "# Используем GPU, если доступен\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Используем взвешенную CrossEntropyLoss\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(train_labels)\n",
    "class_weights = 1. / torch.tensor([counter[i] for i in range(len(class_names))], dtype=torch.float)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Оптимизатор и планировщик\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Используем смешанную точность\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Функция обучения с ранним прекращением\n",
    "def train_model(model, loss_fn, optimizer, scheduler, num_epochs, device, patience=7):\n",
    "    best_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                dataloader = train_dataloader\n",
    "            else:\n",
    "                model.eval()\n",
    "                dataloader = val_dataloader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in tqdm(dataloader, desc=phase):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = loss_fn(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Проверка наилучшей точности\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    epochs_no_improve = 0\n",
    "                    # Сохраняем лучшую модель\n",
    "                    torch.save(model.state_dict(), 'best_model.pth')\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch + 1}')\n",
    "                    print(f'Best val Acc: {best_acc:.4f}')\n",
    "                    return\n",
    "        print()\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "num_epochs = 50\n",
    "train_model(model, loss_fn, optimizer, scheduler, num_epochs, device, patience=7)\n",
    "\n",
    "# Загружаем лучшую модель\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Подготовка тестовых данных\n",
    "test_dir = os.path.join(data_root, 'test')\n",
    "test_image_paths = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir) if fname.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "print(f\"Количество тестовых изображений: {len(test_image_paths)}\")\n",
    "\n",
    "# Используем те же трансформации, что и для валидации\n",
    "test_dataset = CustomImageDataset(test_image_paths, [0]*len(test_image_paths), transforms=get_val_transforms())\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# Используем TTA для предсказаний\n",
    "def get_tta_transforms():\n",
    "    tta_transforms = []\n",
    "    flips = [None, HorizontalFlip(p=1.0), VerticalFlip(p=1.0)]\n",
    "    for flip in flips:\n",
    "        transforms = [Resize(256, 256), CenterCrop(224, 224)]\n",
    "        if flip:\n",
    "            transforms.append(flip)\n",
    "        transforms.extend([\n",
    "            Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        tta_transforms.append(Compose(transforms))\n",
    "    return tta_transforms\n",
    "\n",
    "tta_transforms = get_tta_transforms()\n",
    "\n",
    "# Предсказания на тестовых данных с TTA\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "for idx in tqdm(range(len(test_dataset)), desc='Test'):\n",
    "    images = []\n",
    "    original_image = np.array(Image.open(test_image_paths[idx]).convert(\"RGB\"))\n",
    "    for tta_transform in tta_transforms:\n",
    "        augmented = tta_transform(image=original_image)\n",
    "        image = augmented['image']\n",
    "        images.append(image)\n",
    "    images = torch.stack(images).to(device)\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            outputs = nn.functional.softmax(outputs, dim=1)[:, 1]  # Вероятность класса 'dirty'\n",
    "            probs = outputs.mean().item()\n",
    "    test_predictions.append(probs)\n",
    "\n",
    "# Создание DataFrame для сабмита\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': [os.path.basename(path).replace('.jpg', '').replace('.jpeg', '').replace('.png', '') for path in test_image_paths],\n",
    "    'label': ['dirty' if pred > 0.5 else 'cleaned' for pred in test_predictions]\n",
    "})\n",
    "\n",
    "# Сортировка и сохранение\n",
    "submission_df = submission_df.sort_values('id')\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Файл submission.csv сохранен.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from albumentations import (\n",
    "    Compose, RandomResizedCrop, HorizontalFlip, VerticalFlip, ShiftScaleRotate,\n",
    "    RandomBrightnessContrast, Normalize, CoarseDropout, ColorJitter, GaussianBlur,\n",
    "    GridDistortion, ElasticTransform, RandomGamma, HueSaturationValue, CLAHE,\n",
    "    Resize, CenterCrop, CoarseDropout, OneOf, OpticalDistortion, MotionBlur, MedianBlur,\n",
    "    RandomRain, RandomSnow, RandomFog, RandomShadow, RandomSunFlare, Perspective\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Содержимое data_root: ['cleaned', 'dirty', 'test', 'train', 'val']\n",
      "Количество обучающих изображений: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexx\\AppData\\Local\\Temp\\ipykernel_18048\\4051932503.py:77: UserWarning: Argument 'distortion_scale' is not valid and will be ignored.\n",
      "  Perspective(distortion_scale=0.5, p=0.5),\n",
      "c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B4_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B4_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:01<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.5250\n",
      "Epoch 2/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.6250\n",
      "Epoch 3/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.6250\n",
      "Epoch 4/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7250\n",
      "Epoch 5/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.6750\n",
      "Epoch 6/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7250\n",
      "Epoch 7/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.5750\n",
      "Epoch 8/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.5000\n",
      "Epoch 9/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.6250\n",
      "Epoch 10/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7250\n",
      "Epoch 11/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.6750\n",
      "Epoch 12/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.5750\n",
      "Epoch 13/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.5250\n",
      "Epoch 14/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7500\n",
      "Epoch 15/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.6750\n",
      "Epoch 16/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.6500\n",
      "Epoch 17/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.6750\n",
      "Epoch 18/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.5500\n",
      "Epoch 19/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.6750\n",
      "Epoch 20/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7250\n",
      "Epoch 21/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7500\n",
      "Epoch 22/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7750\n",
      "Epoch 23/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7500\n",
      "Epoch 24/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8500\n",
      "Epoch 25/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8000\n",
      "Epoch 26/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7000\n",
      "Epoch 27/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7250\n",
      "Epoch 28/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8000\n",
      "Epoch 29/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7500\n",
      "Epoch 30/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7000\n",
      "Epoch 31/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7000\n",
      "Epoch 32/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8500\n",
      "Epoch 33/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7500\n",
      "Epoch 34/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8500\n",
      "Epoch 35/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.6500\n",
      "Epoch 36/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7250\n",
      "Epoch 37/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8000\n",
      "Epoch 38/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7000\n",
      "Epoch 39/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8250\n",
      "Epoch 40/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8250\n",
      "Epoch 41/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7750\n",
      "Epoch 42/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7250\n",
      "Epoch 43/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9000\n",
      "Epoch 44/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7500\n",
      "Epoch 45/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7250\n",
      "Epoch 46/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7750\n",
      "Epoch 47/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8500\n",
      "Epoch 48/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8250\n",
      "Epoch 49/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9000\n",
      "Epoch 50/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8750\n",
      "Epoch 51/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8250\n",
      "Epoch 52/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8250\n",
      "Epoch 53/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8750\n",
      "Epoch 54/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7750\n",
      "Epoch 55/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.7500\n",
      "Epoch 56/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8500\n",
      "Epoch 57/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8250\n",
      "Epoch 58/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9000\n",
      "Epoch 59/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9250\n",
      "Epoch 60/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8500\n",
      "Epoch 61/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9250\n",
      "Epoch 62/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9250\n",
      "Epoch 63/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9000\n",
      "Epoch 64/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9250\n",
      "Epoch 65/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9250\n",
      "Epoch 66/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8500\n",
      "Epoch 67/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9000\n",
      "Epoch 68/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8000\n",
      "Epoch 69/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8000\n",
      "Epoch 70/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9250\n",
      "Epoch 71/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8000\n",
      "Epoch 72/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9250\n",
      "Epoch 73/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8750\n",
      "Epoch 74/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8000\n",
      "Epoch 75/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8750\n",
      "Epoch 76/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9000\n",
      "Epoch 77/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8250\n",
      "Epoch 78/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9000\n",
      "Epoch 79/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8250\n",
      "Epoch 80/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8500\n",
      "Epoch 81/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9750\n",
      "Epoch 82/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8250\n",
      "Epoch 83/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9500\n",
      "Epoch 84/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8500\n",
      "Epoch 85/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8250\n",
      "Epoch 86/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9000\n",
      "Epoch 87/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9000\n",
      "Epoch 88/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8250\n",
      "Epoch 89/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9500\n",
      "Epoch 90/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9000\n",
      "Epoch 91/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9000\n",
      "Epoch 92/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9000\n",
      "Epoch 93/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8750\n",
      "Epoch 94/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9500\n",
      "Epoch 95/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9500\n",
      "Epoch 96/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8750\n",
      "Epoch 97/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9250\n",
      "Epoch 98/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9250\n",
      "Epoch 99/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.8000\n",
      "Epoch 100/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5/5 [00:00<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000 Acc: 0.9250\n",
      "Обучение завершено и модель сохранена.\n",
      "Количество тестовых изображений: 744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 744/744 [00:11<00:00, 67.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл submission.csv сохранен.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from albumentations import (\n",
    "    Compose, RandomResizedCrop, HorizontalFlip, VerticalFlip, ShiftScaleRotate,\n",
    "    RandomBrightnessContrast, Normalize, CoarseDropout, ColorJitter, GaussianBlur,\n",
    "    GridDistortion, ElasticTransform, RandomGamma, HueSaturationValue, CLAHE,\n",
    "    Resize, CenterCrop, OneOf, OpticalDistortion, MotionBlur, MedianBlur,\n",
    "    RandomRain, RandomSnow, RandomFog, RandomShadow, RandomSunFlare, Perspective\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Убедитесь, что все необходимые библиотеки установлены\n",
    "# !pip install albumentations\n",
    "\n",
    "# Определяем классы\n",
    "class_names = ['cleaned', 'dirty']\n",
    "\n",
    "# Путь к данным\n",
    "data_root = 'drive/MyDrive/plates'\n",
    "print(\"Содержимое data_root:\", os.listdir(data_root))\n",
    "\n",
    "# Директория с обучающими данными\n",
    "train_dir = os.path.join(data_root, 'train')\n",
    "\n",
    "# Проверяем и создаем необходимые директории\n",
    "for class_name in class_names:\n",
    "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "\n",
    "# Функция для копирования данных в обучающую директорию\n",
    "def prepare_data(source_dir, target_dir):\n",
    "    for class_name in class_names:\n",
    "        class_source_dir = os.path.join(source_dir, class_name)\n",
    "        class_target_dir = os.path.join(target_dir, class_name)\n",
    "        os.makedirs(class_target_dir, exist_ok=True)\n",
    "        files = os.listdir(class_source_dir)\n",
    "        files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        for file_name in files:\n",
    "            shutil.copy(os.path.join(class_source_dir, file_name), os.path.join(class_target_dir, file_name))\n",
    "\n",
    "# Копируем все данные в обучающую директорию\n",
    "prepare_data(data_root, train_dir)\n",
    "\n",
    "# Обновленные аугментации с Albumentations\n",
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "        RandomResizedCrop(224, 224, scale=(0.8, 1.0)),\n",
    "        OneOf([\n",
    "            HorizontalFlip(p=1.0),\n",
    "            VerticalFlip(p=1.0),\n",
    "        ], p=0.5),\n",
    "        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=20, p=0.7),\n",
    "        OneOf([\n",
    "            RandomBrightnessContrast(p=1.0),\n",
    "            ColorJitter(p=1.0),\n",
    "        ], p=0.7),\n",
    "        OneOf([\n",
    "            GaussianBlur(p=1.0),\n",
    "            MotionBlur(p=1.0),\n",
    "            MedianBlur(p=1.0),\n",
    "        ], p=0.5),\n",
    "        OneOf([\n",
    "            RandomRain(p=1.0),\n",
    "            RandomSnow(p=1.0),\n",
    "            RandomFog(p=1.0),\n",
    "            RandomShadow(p=1.0),\n",
    "            RandomSunFlare(p=1.0),\n",
    "        ], p=0.3),\n",
    "        Perspective(distortion_scale=0.5, p=0.5),\n",
    "        CoarseDropout(max_holes=8, max_height=16, max_width=16, fill_value=0, p=0.5),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_test_transforms():\n",
    "    return Compose([\n",
    "        Resize(256, 256),\n",
    "        CenterCrop(224, 224),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "# Кастомный датасет для использования с Albumentations\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка загрузки изображения {self.image_paths[idx]}: {e}\")\n",
    "            image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        label = self.labels[idx]\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image)\n",
    "            image = augmented['image']\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "# Собираем пути к изображениям и метки\n",
    "def get_image_paths_and_labels(data_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            if file_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(class_dir, file_name))\n",
    "                labels.append(label_idx)\n",
    "    return image_paths, labels\n",
    "\n",
    "train_image_paths, train_labels = get_image_paths_and_labels(train_dir)\n",
    "\n",
    "print(f\"Количество обучающих изображений: {len(train_image_paths)}\")\n",
    "\n",
    "# Создаем датасет и загрузчик данных\n",
    "batch_size = 8  # Настройте в зависимости от доступной памяти\n",
    "\n",
    "train_dataset = CustomImageDataset(train_image_paths, train_labels, transforms=get_train_transforms())\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Используем модель EfficientNet-B4\n",
    "model = models.efficientnet_b4(pretrained=True)\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5, inplace=True),\n",
    "    nn.Linear(num_ftrs, len(class_names))\n",
    ")\n",
    "\n",
    "# Размораживаем слои для тонкой настройки\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Используем GPU, если доступен\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Используем Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean', weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(weight=self.weight, reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(focal_loss)\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Используем взвешенную Focal Loss\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(train_labels)\n",
    "class_weights = 1. / torch.tensor([counter[i] for i in range(len(class_names))], dtype=torch.float)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "loss_fn = FocalLoss(weight=class_weights)\n",
    "\n",
    "# Оптимизатор и планировщик\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Функция обучения\n",
    "def train_model(model, loss_fn, optimizer, scheduler, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_dataloader, desc='Train'):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_dataloader.dataset)\n",
    "\n",
    "        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "    # Сохраняем модель после обучения\n",
    "    torch.save(model.state_dict(), 'best_model.pth')\n",
    "    print('Обучение завершено и модель сохранена.')\n",
    "\n",
    "num_epochs = 100  # Вы можете увеличить количество эпох при необходимости\n",
    "train_model(model, loss_fn, optimizer, scheduler, num_epochs, device)\n",
    "\n",
    "# Загружаем обученную модель\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Подготовка тестовых данных\n",
    "test_dir = os.path.join(data_root, 'test')\n",
    "test_image_paths = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir)\n",
    "                    if fname.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "print(f\"Количество тестовых изображений: {len(test_image_paths)}\")\n",
    "\n",
    "# Создаем датасет и загрузчик для тестовых данных\n",
    "test_dataset = CustomImageDataset(test_image_paths, [0]*len(test_image_paths), transforms=get_test_transforms())\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "# Используем TTA для предсказаний\n",
    "def get_tta_transforms():\n",
    "    tta_transforms = []\n",
    "    flips = [None, HorizontalFlip(p=1.0), VerticalFlip(p=1.0)]\n",
    "    for flip in flips:\n",
    "        transforms = [Resize(256, 256), CenterCrop(224, 224)]\n",
    "        if flip:\n",
    "            transforms.append(flip)\n",
    "        transforms.extend([\n",
    "            Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        tta_transforms.append(Compose(transforms))\n",
    "    return tta_transforms\n",
    "\n",
    "tta_transforms = get_tta_transforms()\n",
    "\n",
    "# Предсказания на тестовых данных с TTA\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "for idx in tqdm(range(len(test_dataset)), desc='Test'):\n",
    "    images = []\n",
    "    original_image = np.array(Image.open(test_image_paths[idx]).convert(\"RGB\"))\n",
    "    for tta_transform in tta_transforms:\n",
    "        augmented = tta_transform(image=original_image)\n",
    "        image = augmented['image']\n",
    "        images.append(image)\n",
    "    images = torch.stack(images).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        outputs = nn.functional.softmax(outputs, dim=1)[:, 1]  # Вероятность класса 'dirty'\n",
    "        probs = outputs.mean().item()\n",
    "    test_predictions.append(probs)\n",
    "\n",
    "# Создание DataFrame для сабмита\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': [os.path.basename(path).split('.')[0] for path in test_image_paths],\n",
    "    'label': ['dirty' if pred > 0.5 else 'cleaned' for pred in test_predictions]\n",
    "})\n",
    "\n",
    "# Сортировка и сохранение\n",
    "submission_df = submission_df.sort_values('id')\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Файл submission.csv сохранен.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Содержимое data_root: ['cleaned', 'dirty', 'test', 'train', 'val']\n",
      "Всего изображений: 40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, VerticalFlip, ColorJitter, Normalize,\n",
    "    Resize, ToGray, Perspective\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Определяем классы\n",
    "class_names = ['cleaned', 'dirty']\n",
    "\n",
    "# Путь к данным\n",
    "data_root = 'drive/MyDrive/plates'\n",
    "print(\"Содержимое data_root:\", os.listdir(data_root))\n",
    "\n",
    "# Собираем пути к изображениям и метки\n",
    "def get_image_paths_and_labels(data_root):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_root, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            if file_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(class_dir, file_name))\n",
    "                labels.append(label_idx)\n",
    "    return image_paths, labels\n",
    "\n",
    "image_paths, labels = get_image_paths_and_labels(data_root)\n",
    "print(f\"Всего изображений: {len(image_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "        Resize(256, 256),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        VerticalFlip(p=0.5),\n",
    "        ColorJitter(p=0.5),\n",
    "        ToGray(p=0.2),\n",
    "        Perspective(p=0.5),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_val_transforms():\n",
    "    return Compose([\n",
    "        Resize(256, 256),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[idx]\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image)\n",
    "            image = augmented['image']\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Contents of data_root: ['cleaned', 'dirty', 'test', 'train', 'val']\n",
      "Total images: 40\n",
      "Fold 1/2\n",
      "----------\n",
      "Loaded pretrained weights for efficientnet-b4\n",
      "Epoch 1/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:10<00:00,  5.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7093 Acc: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6793 Acc: 0.5500\n",
      "Val Precision: 1.0000 Recall: 0.1000 F1: 0.1818\n",
      "Improved validation accuracy. Saving model...\n",
      "Epoch 2/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6814 Acc: 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6750 Acc: 0.6000\n",
      "Val Precision: 0.7500 Recall: 0.3000 F1: 0.4286\n",
      "Improved validation accuracy. Saving model...\n",
      "Epoch 3/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5841 Acc: 0.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6887 Acc: 0.5500\n",
      "Val Precision: 0.5714 Recall: 0.4000 F1: 0.4706\n",
      "Epoch 4/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5196 Acc: 0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6841 Acc: 0.6000\n",
      "Val Precision: 0.6667 Recall: 0.4000 F1: 0.5000\n",
      "Epoch 5/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:09<00:00,  4.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4391 Acc: 0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6736 Acc: 0.7000\n",
      "Val Precision: 1.0000 Recall: 0.4000 F1: 0.5714\n",
      "Improved validation accuracy. Saving model...\n",
      "Epoch 6/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4774 Acc: 0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6760 Acc: 0.7000\n",
      "Val Precision: 1.0000 Recall: 0.4000 F1: 0.5714\n",
      "Epoch 7/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4909 Acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6754 Acc: 0.7000\n",
      "Val Precision: 1.0000 Recall: 0.4000 F1: 0.5714\n",
      "Epoch 8/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3361 Acc: 0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6755 Acc: 0.7000\n",
      "Val Precision: 1.0000 Recall: 0.4000 F1: 0.5714\n",
      "Epoch 9/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4971 Acc: 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6735 Acc: 0.7000\n",
      "Val Precision: 1.0000 Recall: 0.4000 F1: 0.5714\n",
      "Epoch 10/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4747 Acc: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6998 Acc: 0.7000\n",
      "Val Precision: 1.0000 Recall: 0.4000 F1: 0.5714\n",
      "Epoch 11/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4486 Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7280 Acc: 0.6000\n",
      "Val Precision: 0.6667 Recall: 0.4000 F1: 0.5000\n",
      "Epoch 12/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6080 Acc: 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7689 Acc: 0.6500\n",
      "Val Precision: 0.6667 Recall: 0.6000 F1: 0.6316\n",
      "Epoch 13/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4892 Acc: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7902 Acc: 0.5000\n",
      "Val Precision: 0.5000 Recall: 0.6000 F1: 0.5455\n",
      "Epoch 14/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6365 Acc: 0.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7670 Acc: 0.5500\n",
      "Val Precision: 0.5455 Recall: 0.6000 F1: 0.5714\n",
      "Epoch 15/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5352 Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7098 Acc: 0.6500\n",
      "Val Precision: 0.6667 Recall: 0.6000 F1: 0.6316\n",
      "Epoch 16/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6645 Acc: 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6673 Acc: 0.7000\n",
      "Val Precision: 0.7500 Recall: 0.6000 F1: 0.6667\n",
      "Epoch 17/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4939 Acc: 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6426 Acc: 0.7000\n",
      "Val Precision: 0.7500 Recall: 0.6000 F1: 0.6667\n",
      "Epoch 18/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4578 Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6297 Acc: 0.7000\n",
      "Val Precision: 0.7500 Recall: 0.6000 F1: 0.6667\n",
      "Epoch 19/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:11<00:00,  5.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6166 Acc: 0.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6237 Acc: 0.7000\n",
      "Val Precision: 0.7500 Recall: 0.6000 F1: 0.6667\n",
      "Epoch 20/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5369 Acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5918 Acc: 0.7000\n",
      "Val Precision: 0.7500 Recall: 0.6000 F1: 0.6667\n",
      "Early stopping due to no improvement in validation accuracy.\n",
      "Best Val Acc for fold 1: 0.7000\n",
      "Fold 2/2\n",
      "----------\n",
      "Loaded pretrained weights for efficientnet-b4\n",
      "Epoch 1/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6978 Acc: 0.1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7317 Acc: 0.4500\n",
      "Val Precision: 0.3333 Recall: 0.1000 F1: 0.1538\n",
      "Improved validation accuracy. Saving model...\n",
      "Epoch 2/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6537 Acc: 0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7661 Acc: 0.4500\n",
      "Val Precision: 0.0000 Recall: 0.0000 F1: 0.0000\n",
      "Epoch 3/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6753 Acc: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]\n",
      "c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.8108 Acc: 0.5000\n",
      "Val Precision: 0.0000 Recall: 0.0000 F1: 0.0000\n",
      "Improved validation accuracy. Saving model...\n",
      "Epoch 4/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7452 Acc: 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n",
      "c:\\Users\\alexx\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.8159 Acc: 0.5000\n",
      "Val Precision: 0.0000 Recall: 0.0000 F1: 0.0000\n",
      "Epoch 5/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4699 Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.8693 Acc: 0.4500\n",
      "Val Precision: 0.0000 Recall: 0.0000 F1: 0.0000\n",
      "Epoch 6/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5532 Acc: 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9400 Acc: 0.4500\n",
      "Val Precision: 0.0000 Recall: 0.0000 F1: 0.0000\n",
      "Epoch 7/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5207 Acc: 0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9957 Acc: 0.4500\n",
      "Val Precision: 0.0000 Recall: 0.0000 F1: 0.0000\n",
      "Epoch 8/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4578 Acc: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.0135 Acc: 0.5000\n",
      "Val Precision: 0.5000 Recall: 0.1000 F1: 0.1667\n",
      "Epoch 9/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5351 Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.0123 Acc: 0.5500\n",
      "Val Precision: 0.6667 Recall: 0.2000 F1: 0.3077\n",
      "Improved validation accuracy. Saving model...\n",
      "Epoch 10/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6666 Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.8751 Acc: 0.6000\n",
      "Val Precision: 0.7500 Recall: 0.3000 F1: 0.4286\n",
      "Improved validation accuracy. Saving model...\n",
      "Epoch 11/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4746 Acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6913 Acc: 0.6000\n",
      "Val Precision: 0.7500 Recall: 0.3000 F1: 0.4286\n",
      "Epoch 12/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4970 Acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5881 Acc: 0.7500\n",
      "Val Precision: 0.7778 Recall: 0.7000 F1: 0.7368\n",
      "Improved validation accuracy. Saving model...\n",
      "Epoch 13/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5372 Acc: 0.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5729 Acc: 0.8000\n",
      "Val Precision: 0.8000 Recall: 0.8000 F1: 0.8000\n",
      "Improved validation accuracy. Saving model...\n",
      "Epoch 14/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5714 Acc: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5875 Acc: 0.8000\n",
      "Val Precision: 0.8000 Recall: 0.8000 F1: 0.8000\n",
      "Epoch 15/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3667 Acc: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6054 Acc: 0.8000\n",
      "Val Precision: 0.8000 Recall: 0.8000 F1: 0.8000\n",
      "Epoch 16/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5034 Acc: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6283 Acc: 0.8000\n",
      "Val Precision: 0.8000 Recall: 0.8000 F1: 0.8000\n",
      "Epoch 17/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4315 Acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6367 Acc: 0.7500\n",
      "Val Precision: 0.7273 Recall: 0.8000 F1: 0.7619\n",
      "Epoch 18/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4799 Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6428 Acc: 0.7500\n",
      "Val Precision: 0.7273 Recall: 0.8000 F1: 0.7619\n",
      "Epoch 19/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6730 Acc: 0.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6454 Acc: 0.7500\n",
      "Val Precision: 0.7273 Recall: 0.8000 F1: 0.7619\n",
      "Epoch 20/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4378 Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6551 Acc: 0.7000\n",
      "Val Precision: 0.7000 Recall: 0.7000 F1: 0.7000\n",
      "Epoch 21/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3546 Acc: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6867 Acc: 0.7000\n",
      "Val Precision: 0.7000 Recall: 0.7000 F1: 0.7000\n",
      "Epoch 22/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5462 Acc: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7717 Acc: 0.6500\n",
      "Val Precision: 0.6667 Recall: 0.6000 F1: 0.6316\n",
      "Epoch 23/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5063 Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.8050 Acc: 0.6500\n",
      "Val Precision: 0.6667 Recall: 0.6000 F1: 0.6316\n",
      "Epoch 24/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5082 Acc: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.8396 Acc: 0.7500\n",
      "Val Precision: 0.7273 Recall: 0.8000 F1: 0.7619\n",
      "Epoch 25/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4886 Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7952 Acc: 0.7500\n",
      "Val Precision: 0.7273 Recall: 0.8000 F1: 0.7619\n",
      "Epoch 26/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4671 Acc: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7806 Acc: 0.7500\n",
      "Val Precision: 0.7273 Recall: 0.8000 F1: 0.7619\n",
      "Epoch 27/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4736 Acc: 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7673 Acc: 0.7500\n",
      "Val Precision: 0.7273 Recall: 0.8000 F1: 0.7619\n",
      "Epoch 28/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:06<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5164 Acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7425 Acc: 0.7500\n",
      "Val Precision: 0.7273 Recall: 0.8000 F1: 0.7619\n",
      "Early stopping due to no improvement in validation accuracy.\n",
      "Best Val Acc for fold 2: 0.8000\n",
      "Number of test images: 744\n",
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting with best_model_fold_0.pth: 100%|██████████| 744/744 [08:47<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting with best_model_fold_1.pth: 100%|██████████| 744/744 [08:39<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission.csv\n",
      "     id    label\n",
      "0  0000    dirty\n",
      "1  0001    dirty\n",
      "2  0002    dirty\n",
      "3  0003    dirty\n",
      "4  0004  cleaned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "# Uncomment the following lines if you're running in a new environment\n",
    "# !pip install albumentations\n",
    "# !pip install efficientnet_pytorch\n",
    "# !pip install timm\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, VerticalFlip, ColorJitter, ToGray, Perspective, Normalize,\n",
    "    ShiftScaleRotate, RandomBrightnessContrast, CoarseDropout\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import random\n",
    "import copy\n",
    "import timm\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Define the class names\n",
    "class_names = ['cleaned', 'dirty']\n",
    "\n",
    "# Define the data root directory\n",
    "data_root = 'drive/MyDrive/plates'  # Replace with your actual data directory\n",
    "print(\"Contents of data_root:\", os.listdir(data_root))\n",
    "\n",
    "# Image size for EfficientNet-B4\n",
    "image_size = 380\n",
    "\n",
    "# Define transformations using Albumentations\n",
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "        # We remove Resize here because we'll resize images before transformations\n",
    "        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        VerticalFlip(p=0.5),\n",
    "        RandomBrightnessContrast(p=0.5),\n",
    "        ColorJitter(p=0.5),\n",
    "        ToGray(p=0.1),\n",
    "        Perspective(p=0.5),\n",
    "        CoarseDropout(max_height=32, max_width=32, max_holes=5, p=0.5),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_val_transforms():\n",
    "    return Compose([\n",
    "        # We remove Resize here because we'll resize images before transformations\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transforms=None, use_mixup=False, use_cutmix=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "        self.use_mixup = use_mixup\n",
    "        self.use_cutmix = use_cutmix\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load and resize the primary image\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (image_size, image_size))\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.use_mixup or self.use_cutmix:\n",
    "            # Load and resize the secondary image\n",
    "            idx2 = random.randint(0, len(self.image_paths) - 1)\n",
    "            image2 = cv2.imread(self.image_paths[idx2])\n",
    "            image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "            image2 = cv2.resize(image2, (image_size, image_size))\n",
    "            label2 = self.labels[idx2]\n",
    "\n",
    "            if self.use_cutmix:\n",
    "                # Apply CutMix\n",
    "                lam = np.random.beta(1.0, 1.0)\n",
    "                bbx1, bby1, bbx2, bby2 = self.rand_bbox(image.shape[0], image.shape[1], lam)\n",
    "                image[bbx1:bbx2, bby1:bby2, :] = image2[bbx1:bbx2, bby1:bby2, :]\n",
    "                label = label * lam + label2 * (1. - lam)\n",
    "            elif self.use_mixup:\n",
    "                # Apply Mixup\n",
    "                lam = np.random.beta(1.0, 1.0)\n",
    "                image = (image * lam + image2 * (1 - lam)).astype(np.uint8)\n",
    "                label = label * lam + label2 * (1. - lam)\n",
    "\n",
    "            if self.transforms:\n",
    "                augmented = self.transforms(image=image)\n",
    "                image = augmented['image']\n",
    "        else:\n",
    "            if self.transforms:\n",
    "                augmented = self.transforms(image=image)\n",
    "                image = augmented['image']\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def rand_bbox(self, H, W, lam):\n",
    "        cut_rat = np.sqrt(1. - lam)\n",
    "        cut_w = int(W * cut_rat)\n",
    "        cut_h = int(H * cut_rat)\n",
    "\n",
    "        # uniform\n",
    "        cx = np.random.randint(W)\n",
    "        cy = np.random.randint(H)\n",
    "\n",
    "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "        return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# Function to get image paths and labels\n",
    "def get_image_paths_and_labels(data_root):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_root, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            if file_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(class_dir, file_name))\n",
    "                labels.append(label_idx)\n",
    "    return image_paths, labels\n",
    "\n",
    "# Prepare the dataset\n",
    "image_paths, labels = get_image_paths_and_labels(data_root)\n",
    "print(f\"Total images: {len(image_paths)}\")\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "k_folds = 2\n",
    "use_mixup = True\n",
    "use_cutmix = True\n",
    "\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "models_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(image_paths, labels)):\n",
    "    print(f'Fold {fold + 1}/{k_folds}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Create datasets for the current fold\n",
    "    train_dataset = CustomImageDataset(\n",
    "        [image_paths[i] for i in train_idx],\n",
    "        [labels[i] for i in train_idx],\n",
    "        transforms=get_train_transforms(),\n",
    "        use_mixup=use_mixup,\n",
    "        use_cutmix=use_cutmix\n",
    "    )\n",
    "    val_dataset = CustomImageDataset(\n",
    "        [image_paths[i] for i in val_idx],\n",
    "        [labels[i] for i in val_idx],\n",
    "        transforms=get_val_transforms(),\n",
    "        use_mixup=False,\n",
    "        use_cutmix=False  # We don't use Mixup/CutMix in validation\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "    num_ftrs = model._fc.in_features\n",
    "    model._fc = nn.Linear(num_ftrs, 1)  # For BCEWithLogitsLoss\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    # Training loop\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    patience_counter = 0\n",
    "    early_stop_patience = 15\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Train phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels_batch in tqdm(train_loader, desc='Training'):\n",
    "            inputs = inputs.to(device)\n",
    "            labels_batch = labels_batch.to(device).float().unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels_batch)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(epoch + len(train_loader) / len(train_loader))\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels_batch.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels_batch in tqdm(val_loader, desc='Validation'):\n",
    "                inputs = inputs.to(device)\n",
    "                labels_batch = labels_batch.to(device).float().unsqueeze(1)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels_batch)\n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels_batch.data)\n",
    "\n",
    "                all_labels.extend(labels_batch.cpu().numpy())\n",
    "                all_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "\n",
    "        val_loss_epoch = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "        print(f'Val Loss: {val_loss_epoch:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Calculate additional metrics\n",
    "        all_labels_np = np.array(all_labels)\n",
    "        all_preds_np = np.array(all_preds)\n",
    "        val_precision = precision_score(all_labels_np, all_preds_np > 0.5)\n",
    "        val_recall = recall_score(all_labels_np, all_preds_np > 0.5)\n",
    "        val_f1 = f1_score(all_labels_np, all_preds_np > 0.5)\n",
    "        print(f'Val Precision: {val_precision:.4f} Recall: {val_recall:.4f} F1: {val_f1:.4f}')\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "            print('Improved validation accuracy. Saving model...')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stop_patience:\n",
    "                print('Early stopping due to no improvement in validation accuracy.')\n",
    "                break\n",
    "\n",
    "    print(f'Best Val Acc for fold {fold + 1}: {best_acc:.4f}')\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    # Save the best model for this fold\n",
    "    fold_model_path = f'best_model_fold_{fold}.pth'\n",
    "    torch.save(model.state_dict(), fold_model_path)\n",
    "    models_list.append(fold_model_path)\n",
    "\n",
    "# Prepare test data\n",
    "test_dir = os.path.join(data_root, 'test')\n",
    "test_image_paths = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir)\n",
    "                    if fname.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "print(f\"Number of test images: {len(test_image_paths)}\")\n",
    "\n",
    "# Test-Time Augmentation (TTA)\n",
    "def tta_inference(model, image):\n",
    "    tta_transforms = []\n",
    "    tta_transforms.append(get_val_transforms())\n",
    "    tta_transforms.append(Compose([A.HorizontalFlip(p=1.0)] + get_val_transforms().transforms))\n",
    "    tta_transforms.append(Compose([A.VerticalFlip(p=1.0)] + get_val_transforms().transforms))\n",
    "    tta_transforms.append(Compose([A.Transpose(p=1.0)] + get_val_transforms().transforms))\n",
    "\n",
    "    preds = []\n",
    "    for transformer in tta_transforms:\n",
    "        img = cv2.resize(image, (image_size, image_size))\n",
    "        augmented = transformer(image=img)\n",
    "        img = augmented['image'].unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "            prob = torch.sigmoid(output).cpu().numpy()\n",
    "            preds.append(prob[0][0])\n",
    "    return np.mean(preds)\n",
    "\n",
    "# Ensemble predictions\n",
    "ensemble_preds = np.zeros(len(test_image_paths))\n",
    "\n",
    "for model_path in models_list:\n",
    "    model = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "    num_ftrs = model._fc.in_features\n",
    "    model._fc = nn.Linear(num_ftrs, 1)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    for idx, img_path in enumerate(tqdm(test_image_paths, desc=f'Predicting with {model_path}')):\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        pred = tta_inference(model, image)\n",
    "        preds.append(pred)\n",
    "\n",
    "    ensemble_preds += np.array(preds)\n",
    "\n",
    "# Average the predictions\n",
    "ensemble_preds /= len(models_list)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': [os.path.basename(path).split('.')[0] for path in test_image_paths],\n",
    "    'label': ['dirty' if pred > 0.5 else 'cleaned' for pred in ensemble_preds]\n",
    "})\n",
    "\n",
    "# Sort and save the submission\n",
    "submission_df = submission_df.sort_values('id')\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Saved submission.csv\")\n",
    "\n",
    "# Optionally, you can print the final submission DataFrame\n",
    "print(submission_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
