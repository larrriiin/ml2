{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "spqfuGsBtwSd"
      },
      "outputs": [],
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?а-яА-Я]+\", r\" \", s)\n",
        "    return s"
      ],
      "metadata": {
        "id": "ADEsl1uSt5QJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = open('rus.txt', encoding='utf-8').read().strip().split('\\n')\n",
        "pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
        "\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")"
      ],
      "metadata": {
        "id": "tQmHyJvvt-tf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "           len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "           p[1].startswith(eng_prefixes)\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'rus', True)\n",
        "\n",
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def initHiddenCell(self):\n",
        "        hidden = torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
        "        cell = torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
        "        return hidden, cell\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, num_layers=1):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, (hidden, cell) = self.lstm(output, (hidden, cell))\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def initHiddenCell(self):\n",
        "        hidden = torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
        "        cell = torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
        "        return hidden, cell\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return input_tensor, target_tensor"
      ],
      "metadata": {
        "id": "q-UDTlP4uBg3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden, encoder_cell = encoder.initHiddenCell()\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    loss = 0\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
        "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "    if use_teacher_forcing:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]\n",
        "    else:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    return loss.item() / target_length\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden, encoder_cell = encoder.initHiddenCell()\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
        "        decoded_words = []\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "        return decoded_words\n",
        "\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.01):\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs)) for _ in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        if iter % print_every == 0:\n",
        "            print(f\"Iteration {iter}: Loss = {loss}\")"
      ],
      "metadata": {
        "id": "-vQKNGJYufMY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderLSTM(input_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = DecoderLSTM(hidden_size, output_lang.n_words).to(device)"
      ],
      "metadata": {
        "id": "m0hWOV1CuiPY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainIters(encoder1, decoder1, 75000, print_every=5000)\n",
        "\n",
        "# Функция для случайной проверки модели\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "\n",
        "# Проверим качество перевода на случайных примерах\n",
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3oIH25ouodv",
        "outputId": "fc98be3d-6730-42dd-ea17-12fe0e8b3655"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5000: Loss = 0.017588727176189423\n",
            "Iteration 10000: Loss = 0.00445779071499904\n",
            "Iteration 15000: Loss = 0.002574014477431774\n",
            "Iteration 20000: Loss = 0.0012947161061068375\n",
            "Iteration 25000: Loss = 0.0008875545114278794\n",
            "Iteration 30000: Loss = 0.0007747888254622618\n",
            "Iteration 35000: Loss = 0.0006126898806542158\n",
            "Iteration 40000: Loss = 0.00042873923666775227\n",
            "Iteration 45000: Loss = 0.0008884982671588659\n",
            "Iteration 50000: Loss = 0.0006796061061322689\n",
            "Iteration 55000: Loss = 0.0005874824710190296\n",
            "Iteration 60000: Loss = 0.0002721909841056913\n",
            "Iteration 65000: Loss = 0.0004858759348280728\n",
            "Iteration 70000: Loss = 0.00033326169941574337\n",
            "Iteration 75000: Loss = 0.0002750233979895711\n",
            "> it is too expensive .\n",
            "= это слишком дорого .\n",
            "< это слишком дорого . <EOS>\n",
            "\n",
            "> goodbye !\n",
            "= до свидания !\n",
            "< до свидания ! <EOS>\n",
            "\n",
            "> i need help .\n",
            "= мне нужна помощь .\n",
            "< мне нужна помощь . <EOS>\n",
            "\n",
            "> good night !\n",
            "= спокоинои ночи !\n",
            "< спокоинои ночи ! <EOS>\n",
            "\n",
            "> this is my house .\n",
            "= это мои дом .\n",
            "< это мои дом . <EOS>\n",
            "\n",
            "> please repeat this .\n",
            "= пожалуиста повторите это .\n",
            "< пожалуиста повторите это . <EOS>\n",
            "\n",
            "> thank you for your help .\n",
            "= спасибо за помощь .\n",
            "< спасибо за помощь . <EOS>\n",
            "\n",
            "> i love reading books .\n",
            "= я люблю читать книги .\n",
            "< я люблю читать книги . <EOS>\n",
            "\n",
            "> good morning !\n",
            "= доброе утро !\n",
            "< доброе утро ! <EOS>\n",
            "\n",
            "> please repeat this .\n",
            "= пожалуиста повторите это .\n",
            "< пожалуиста повторите это . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"i am learning russian .\"\n",
        "print(evaluate(encoder1, decoder1, sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pqT2xRLvHb1",
        "outputId": "08d0da7a-5ba1-4e5c-f587-0454ac288f54"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['я', 'изучаю', 'русскии', 'язык', '.', '<EOS>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QDY8gPg-_kcE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}